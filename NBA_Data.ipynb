{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_files():\n",
    "\n",
    "    url = 'https://www.basketball-reference.com/leagues/' \n",
    "\n",
    "    seasons = ['NBA_2011_per_game.html', 'NBA_2012_per_game.html', 'NBA_2013_per_game.html', 'NBA_2014_per_game.html', 'NBA_2015_per_game.html',\n",
    "            'NBA_2016_per_game.html', 'NBA_2017_per_game.html', 'NBA_2018_per_game.html', 'NBA_2019_per_game.html', 'NBA_2020_per_game.html',\n",
    "            'NBA_2021_per_game.html', 'NBA_2022_per_game.html', 'NBA_2023_per_game.html', 'NBA_2024_per_game.html']\n",
    "    for season in seasons:\n",
    "        base_url = url + season\n",
    "        dir_path = '/Users/josephcrawford/Documents/data_engineering/test'\n",
    "\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        try:\n",
    "            response = requests.get(base_url)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print('HTTP Error')\n",
    "            print(e)\n",
    "        with open(os.path.join(dir_path, season) ,'w', encoding='utf-8') as file:\n",
    "                file.write(response.text)\n",
    "get_html_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(file_path, season):\n",
    "   \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    tables = soup.find_all('table')\n",
    "    dataframes = []\n",
    "\n",
    "    for table in tables:\n",
    "        dfs = pd.read_html(str(table))\n",
    "        for df in dfs:\n",
    "            df['SEASON'] = season\n",
    "            dataframes.extend(dfs)\n",
    "\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_htmls(working_directory):   \n",
    "    processed_dataframes = []\n",
    "    \n",
    "    for file in os.listdir(working_directory):\n",
    "        if file.endswith('.html'):\n",
    "            file_path = os.path.join(working_directory, file)\n",
    "            season = file.split('_')[1]\n",
    "            dataframes = parse_html(file_path, season)\n",
    "            processed_dataframes.extend(dataframes)\n",
    "   \n",
    "\n",
    "    combined_dfs = pd.concat(processed_dataframes, ignore_index=False)\n",
    "\n",
    "    return combined_dfs\n",
    "\n",
    "\n",
    "working_directory = '/Users/josephcrawford/Documents/data_engineering/NBA_Stats' \n",
    "\n",
    "combined_dfs = process_htmls(working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = combined_dfs[~combined_dfs['Rk'].str.contains('Rk')]\n",
    "df2 = df2.drop('Rk', axis=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
